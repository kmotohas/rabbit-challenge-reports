# DNN

## Day1

### Section 1: 入力層・中間層

中間層のユニット数がひとつのネットワークに対して以下の設定を考える。

- 入力のベクトル $\mathbf{x}$
- 入力層と中間層の接続の重み $\mathbf{w}$
- 入力層と中間層の接続のバイアス $\mathbf{b}$
- 中間層のユニットの総入力 $u$
- 中間層の出力 $z$
- 活性化関数 $f$

このとき、中間層の総入力 $u$ は次の式で与えられる。

$$
u=\mathbf{wx}+\mathbf{b}
$$

なお、上式を `numpy` を用いて実装すると次のようになる。

```python:
import numpy as np
u = np.dot(w, x) + b
```

### Section 2: 活性化関数

ニューラルネットワークにおいて、次の層への出力の大きさを決める非線形の関数のこと。入力値によって、次の層への信号のON/OFFや強弱を定める働きを持つ。

- 中間層用によく使われる活性化関数
  - ReLU関数
  - シグモイド（ロジスティック）関数
  - ステップ関数
- 出力層用によく使われる活性化関数
  - ソフトマックス関数
  - 恒等写像
  - シグモイド（ロジスティック）関数

### Section 3: 出力層

分類問題を解きたいとき事前に用意するデータとしては入力データと分類の正解ラベルがある。ニューラルネットワークから出力される値と正解との差を評価するために誤差関数が用いられる。誤差関数には例えば二乗誤差がある。

$$
E_n(\mathbf{w})=\frac{1}{2}\sum^J_{j=1}(y_j-d_j)^2=\frac{1}{2}||(\mathbf{y}-\mathbf{d})||^2
$$

それぞれの問題で用いられる出力層の活性化関数と誤差関数の組み合わせは以下の表の通り。

||回帰|二値分類|多クラス分類|
|-----|-----|------|-----|
|活性化関数|恒等写像|シグモイド関数|ソフトマックス関数|
|誤差関数|二乗誤差|交差エントロピー|交差エントロピー|

### Section 4: 勾配降下法

勾配降下法では以下の式に従ってパラメータを最適化する。

$$
w^{(t+1)}=w^{(t)}-\varepsilon\nabla E
$$

ただし、$\varepsilon$ は学習率、$E$ は誤差の値とする。学習率が多すぎると最小値にいつまでもたどり着かず発散してしまうことがある。学習率が小さいと発散することはないが、小さすぎると収束するまでに時間がかかってしまう。勾配降下法のアルゴリズムには以下のようなものがある。

- Momentum
- AdaGrad
- Adadelta
- Adam

勾配降下法は全サンプルの平均誤差を用いるのに対して、確率的勾配降下法（SGD）はランダムに抽出したサンプルの誤差を用いる。メリットは以下の通り。

- データが冗長な場合の計算コストの軽減
- 望まない局所極少解に収束するリスクの軽減
- オンライン学習ができる

これらの中間的な方法として、ランダムに分割したデータの集合に属するサンプルの平均誤差を用いるミニバッチ勾配降下法がある。ミニバッチ勾配降下法のメリットには次のようなものがある。

- 確率的勾配降下法のメリットを損なわず、計算機の資源を有効活用できる
  - CPUを利用したスレッド並列化やGPUを利用したSIMD並列化

### Section 5: 誤差逆伝播法

算出された誤差を、出力層側から順に微分し、前の層前の層へと伝播。最小限の計算で各パラメータでの微分値を解析的に計算する手法。誤差から微分を逆算することで、不要な再帰的計算を避けて微分を算出できる。

$$
\frac{\partial E}{\partial w_{ji}}
=\frac{\partial E}{\partial y}
 \frac{\partial y}{\partial u}
 \frac{\partial u}{\partial w_{ji}}
$$

出力層の活性化関数は恒等写像、誤差関数を二乗誤差とすると、それぞれの項は以下のように計算できる。

$$
\frac{\partial E}{\partial y}=y-d
$$

$$
\frac{\partial y}{\partial u}=1
$$

$$
\frac{\partial u}{\partial w_{ji}}=(y_j-d_j)z_i
$$

## Day2

### Section 1: 勾配消失問題について

### Section 2: 学習率最適化手法について

### Section 3: 過学習について

### Section 4: 畳み込みニューラルネットワークの概念

### Section 5: 最新のCNN

## Day3

### Section 1: 再帰型ニューラルネットワークの概念

### Section 2:

### Section 3:

### Section 4:

### Section 5:

### Section 6:

### Section 7:

## Day4

### Section 1: TensorFlowの実装演習

### Section 2: 強化学習
