# 機械学習レポート

学習モデリングプロセスは以下の通り。

1. 問題設定
2. データ選定
3. データの前処理
4. 機械学習モデルの選定
5. モデルの学習（パラメータ推定
6. モデルの評価

## 第1章 線形回帰モデル

回帰とは、ある数値入力から連続値の出力を予測する問題である。 線形回帰は回帰問題を解くための機械学習モデルのひとつ。 入力と $m$ 次元パラメータの線型結合を出力する。
$$
\hat{y}=\mathbf{w}^\mathrm{T}\mathbf{x}+b
$$
データが複数あれば連立方程式を立てることができ、行列表現では以下のように書ける。
$$
\mathbf{y}=X\mathrm{w}+\mathbf{\varepsilon}
$$
ただし、$X=(\mathbf{x}_1,\cdots,\mathbf{x}_n)^\mathrm{T}$, $\mathbf{\varepsilon}=(\mathbf{\varepsilon}_1,\cdots,\mathbf{\varepsilon}_n)^\mathrm{T}$ であり、$\mathbf{\varepsilon}$ は回帰直面に加わる誤差とする。

パラメータの推定はデータとモデル出力の平均自乗誤差 (Mean Squared Error; MSE) をパラメータで微分し、0となる $\mathbf{w}$ の点を求めることで行われる。回帰係数の解は以下の式で与えられる。
$$
\hat{\mathbf{w}}=(X^\text{(train)T}X^\text{(train)})^{-1}X^\text{(train)T}\mathbf{y}^\text{(train)}
$$

### ハンズオン：ボストン住宅価格データ

## 第2章 非線形回帰モデル

複雑な非線形構造を内在する現象に対しては、非線形回帰モデリングを実施する。 方法のひとつに基底展開法と呼ばれるものがある。
$$
y_i=w_0+\sum^m_{j=1}w_j\phi_j(\mathbf{x}_i)+\varepsilon_i
$$
ここで $\phi$ は基底関数であり、多項式函数、ガウス型基底関数、スプライン関数、Bスプライン関数などがよく用いられる。未知パラメータは最小自乗法や最尤法により推定される。

複雑な関数をフィットするとき過学習 (overfitting) や、未学習 (underfitting) といった問題が見られることがある。過学習は正則化法で回避しうる。モデルの複雑さに伴って、その値が大きくなるペナルティ項を追加した上で最小化問題を解く。正則化項の係数である正則化パラメータはクロスバリデーションで選択する。

## 第3章 ロジスティック回帰モデル

回帰という名前がついているが、ある数値入力からクラスに分類する「分類問題」に用いられる。
線型結合の結果をロジスティック関数の入力とし、0以上1以下の値を出力する。出力は確率として解釈できる。ロジスティック関数（シグモイド関数とも）の形は以下の通り。
$$
\sigma(x)=\frac{1}{1+\exp(-ax)}
$$
シグモイド関数の微分は、自身で表現することが可能という便利な性質がある。
$$
\sigma^\prime=a\sigma(x)(1-\sigma(x))
$$

モデルの出力が $Y=t$ となる確率はベルヌーイ分布で表すことができる。
$$
P(Y=t|\mathbf{x})=p^t(1-p)^{1-t}
$$
パラメータ $\mathbf{w}$ は負の対数尤度関数を最小化するように推定される。
$$
E(w_0,\cdots,w_m)=-\log L(w_0,\cdots,w_m)=-\sum^n_{i=1}(t_n\log p_i+(1-t_n)\log(1-p_i))
$$
解析解を求めるのは困難なので勾配降下法 (Gradient descent) を用いて、反復学習によりパラメータを逐次的に更新していく。

分類タスクの評価指標としては以下の値がよく用いられる。
- 正解率
- 適合率
- 再現率
- F値

### ハンズオン：タイタニック

## 第4章 主成分分析

多変量データの持つ構造をより少数個の指標にまとめる次元削減の手法。学習データの分散が最大になる方向への線形変換を求める。具体的には、線形変換に用いる係数ベクトルのノルムが1になる制約のもとラグランジュ未定乗数法を用いて変換後の分散が最大になる条件を求める。これは分散共分散行列の固有値・固有ベクトルを求めることに一致する。

## 第5章 アルゴリズム

- k近傍法 (kNN)
  - 分類問題のための機械学習手法
  - 近傍k個のデータのクラスラベルの中で最も多いラベルを割り当てる
  - kを大きくすると決定境界は滑らかになる
- k平均法 (k-means)
  - 教師なし学習のクラスタリング手法
  - 与えられたデータをk個のクラスタに分類する
  - まず、各クラスタ中心の初期値をランダムに設定する
    - 各データ点を中心との距離が最も近いクラスタに割り当てる。
    - そして、再度クラスタの重心を計算する。
    - 収束するまでこの処理を繰り返す。

## 第6章 Appendix

ただの参考文献なので省略。

## 第7章 サポートベクターマシン

学習ビデオが提供されていないので省略。

### ハンズオン：タイタニック（SVM)
